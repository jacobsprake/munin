"""
Compare Munin Performance Metrics vs Baseline
Demonstrates coordination latency reduction.
"""
import json
from pathlib import Path
from datetime import datetime
import time


def load_packet_metrics(packet_path: Path) -> dict:
    """Extract timing metrics from a packet."""
    with open(packet_path, 'r') as f:
        packet = json.load(f)
    
    created_ts = datetime.fromisoformat(packet['createdTs'].replace('Z', '+00:00'))
    
    # Check for approval timestamps
    approvals = packet.get('approvals', [])
    first_approval_ts = None
    for approval in approvals:
        if approval.get('signedTs'):
            first_approval_ts = datetime.fromisoformat(approval['signedTs'].replace('Z', '+00:00'))
            break
    
    metrics = {
        'packet_id': packet['id'],
        'created_at': created_ts,
        'status': packet['status'],
        'playbook': packet.get('playbookId', 'N/A'),
        'first_approval_at': first_approval_ts,
        'time_to_authorize': None,
        'multi_sig_status': packet.get('multiSig', {}),
        'byzantine_status': packet.get('byzantineMultiSig', {})
    }
    
    if first_approval_ts:
        metrics['time_to_authorize'] = (first_approval_ts - created_ts).total_seconds()
    
    return metrics


def compare_metrics(packets_dir: Path):
    """
    Compare Munin metrics vs baseline.
    """
    print("=" * 60)
    print("MUNIN vs BASELINE METRICS COMPARISON")
    print("=" * 60)
    
    # Load all packets (prefer approved versions)
    packet_files = list(packets_dir.glob("packet_*.json"))
    if not packet_files:
        print("‚ùå No packets found")
        return
    
    print(f"\nüì¶ Found {len(packet_files)} packet(s)")
    
    # Extract metrics (prefer approved versions)
    all_metrics = []
    processed_ids = set()
    for packet_file in sorted(packet_files, key=lambda x: '_approved' in x.name, reverse=True):
        packet_id = packet_file.stem.replace('_approved', '')
        if packet_id in processed_ids:
            continue  # Skip if we already processed the approved version
        processed_ids.add(packet_id)
        metrics = load_packet_metrics(packet_file)
        all_metrics.append(metrics)
    
    # Filter for flood coordination packets
    flood_metrics = [m for m in all_metrics if 'flood' in m['playbook'].lower() or 'carlisle' in m['playbook'].lower()]
    
    if not flood_metrics:
        print("‚ö†Ô∏è  No flood coordination packets found")
        return
    
    print(f"\nüåä Flood Coordination Packets: {len(flood_metrics)}")
    
    # Baseline metrics (traditional phone-tree/PDF/email coordination process)
    # Based on: incident detection (5-15min) + cross-agency calls (30-60min) + 
    # legal review (30-90min) + multi-ministry approval (60-120min) + execution (5-10min)
    # Total: 2-6 hours (130-295 minutes)
    BASELINE_METRICS = {
        'time_to_authorize': {
            'min': 2 * 3600,  # 2 hours (best case)
            'max': 6 * 3600,  # 6 hours (worst case, complex coordination)
            'avg': 4 * 3600   # 4 hours average (typical multi-agency coordination)
        },
        'time_to_task': {
            'min': 30 * 60,   # 30 minutes
            'max': 60 * 60,   # 1 hour
            'avg': 45 * 60    # 45 minutes average
        },
        'coordination_latency': {
            'min': 1 * 3600,  # 1 hour
            'max': 3 * 3600,  # 3 hours
            'avg': 2 * 3600   # 2 hours average
        }
    }
    
    # Munin target metrics
    MUNIN_TARGETS = {
        'time_to_authorize': 2 * 60,      # < 2 minutes
        'time_to_task': 1 * 60,           # < 1 minute
        'coordination_latency': 5 * 60    # < 5 minutes
    }
    
    print(f"\n" + "=" * 60)
    print("METRICS COMPARISON")
    print("=" * 60)
    
    for metrics in flood_metrics:
        print(f"\nüìã Packet: {metrics['packet_id']}")
        print(f"   Playbook: {metrics['playbook']}")
        print(f"   Status: {metrics['status']}")
        
        # Time-to-authorize comparison
        print(f"\n‚è±Ô∏è  TIME-TO-AUTHORIZE")
        if metrics['time_to_authorize']:
            munin_time = metrics['time_to_authorize']
            baseline_min = BASELINE_METRICS['time_to_authorize']['min']
            baseline_max = BASELINE_METRICS['time_to_authorize']['max']
            baseline_avg = BASELINE_METRICS['time_to_authorize']['avg']
            target = MUNIN_TARGETS['time_to_authorize']
            
            print(f"   Munin: {munin_time:.1f} seconds ({munin_time/60:.2f} minutes)")
            print(f"   Target: < {target/60:.0f} minutes")
            print(f"   Baseline: {baseline_min/3600:.0f}-{baseline_max/3600:.0f} hours (avg: {baseline_avg/3600:.0f} hours)")
            print(f"   Baseline breakdown: detection (5-15min) + coordination (30-60min) + legal review (30-90min) + approval (60-120min)")
            
            if munin_time <= target:
                print(f"   ‚úÖ MEETS TARGET (< {target/60:.0f} minutes)")
            else:
                print(f"   ‚ö†Ô∏è  Exceeds target by {(munin_time - target)/60:.2f} minutes")
            
            improvement_vs_baseline = ((baseline_avg - munin_time) / baseline_avg) * 100
            print(f"   üìà Improvement vs baseline: {improvement_vs_baseline:.1f}% faster")
            print(f"   ‚ö° Speedup: {baseline_avg/munin_time:.1f}x faster")
        else:
            print(f"   ‚è≥ Not yet authorized")
        
        # Multi-sig status
        print(f"\nüîê MULTI-SIGNATURE STATUS")
        multi_sig = metrics['multi_sig_status']
        byzantine = metrics['byzantine_status']
        print(f"   Standard: {multi_sig.get('currentSignatures', 0)}/{multi_sig.get('threshold', 0)}")
        print(f"   Byzantine: {byzantine.get('currentSignatures', 0)}/{byzantine.get('threshold', 0)}")
        print(f"   Required Ministries: {', '.join(byzantine.get('requiredMinistries', []))}")
    
    # Summary comparison
    print(f"\n" + "=" * 60)
    print("SUMMARY COMPARISON")
    print("=" * 60)
    
    authorized_metrics = [m for m in flood_metrics if m['time_to_authorize']]
    if authorized_metrics:
        avg_munin_time = sum(m['time_to_authorize'] for m in authorized_metrics) / len(authorized_metrics)
        baseline_avg = BASELINE_METRICS['time_to_authorize']['avg']
        
        print(f"\nüìä Average Time-to-Authorize:")
        print(f"   Munin: {avg_munin_time/60:.2f} minutes")
        print(f"   Baseline: {baseline_avg/3600:.0f} hours")
        print(f"   Improvement: {((baseline_avg - avg_munin_time) / baseline_avg) * 100:.1f}% faster")
        print(f"   Speedup: {baseline_avg/avg_munin_time:.1f}x faster")
    
    print(f"\nüéØ Key Metrics:")
    print(f"   Time-to-authorize: Munin < 2 min vs Baseline 2-6 hours")
    print(f"   Time-to-task: Munin < 1 min vs Baseline 30-60 min")
    print(f"   Coordination latency: Munin < 5 min vs Baseline 1-3 hours")
    print(f"\nüí° The Carlisle demo validates Munin's coordination latency reduction thesis:")
    print(f"   Not better prediction, but faster authorized execution.")


if __name__ == "__main__":
    script_dir = Path(__file__).parent
    packets_dir = script_dir / "out" / "carlisle_demo" / "packets"
    
    if not packets_dir.exists():
        print(f"‚ùå Packets directory not found: {packets_dir}")
        print("   Run carlisle_demo.py first to generate packets")
        exit(1)
    
    compare_metrics(packets_dir)
